{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "from datetime import datetime\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(layers.Layer):\n",
    "\n",
    "    def __init__(self, encoding_layer_size):\n",
    "        super().__init__()\n",
    "        self.encoding_layer = layers.Dense(\n",
    "            units=encoding_layer_size,\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer='random_uniform',\n",
    "            bias_initializer='zeros'\n",
    "        )\n",
    "        self.output_layer = layers.Dense(\n",
    "        units=encoding_layer_size,\n",
    "        activation=tf.nn.sigmoid\n",
    "        )\n",
    "\n",
    "    def call(self, input_attributes):\n",
    "        activation = self.encoding_layer(input_attributes)\n",
    "        return self.output_layer(activation)\n",
    "\n",
    "class Autodecoder(layers.Layer):\n",
    "    def __init__(self, encoding_layer_size, attributes_size):\n",
    "        super().__init__()\n",
    "        self.decoding_layer = layers.Dense(\n",
    "            units=encoding_layer_size,\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer='he_normal',\n",
    "            bias_initializer='zeros'\n",
    "        )\n",
    "        self.output_layer = layers.Dense(\n",
    "            units=attributes_size,\n",
    "            activation=tf.nn.sigmoid\n",
    "        )\n",
    "    def call(self, encoded):\n",
    "        activation = self.decoding_layer(encoded)\n",
    "        return self.output_layer(activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderModel(tf.keras.Model):\n",
    "    def __init__(self, encoding_layer_size, attributes_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Autoencoder(encoding_layer_size=encoding_layer_size)\n",
    "        self.decoder = Autodecoder(\n",
    "            encoding_layer_size=encoding_layer_size, attributes_size=attributes_size\n",
    "        )\n",
    "    def call(self, input_attributes):\n",
    "        encoded = self.encoder(input_attributes)\n",
    "        reconstructed = self.decoder(encoded)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, original):\n",
    "  reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original)))\n",
    "  return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_step(loss, model, optamizer, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients = tape.gradient(loss(autoencoder, target), autoencoder.trainable_variables)\n",
    "        gradient_variables = zip(gradients, autoencoder.trainable_variables)\n",
    "        optamizer.apply_gradients(gradient_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "encoding_layer_size = 100\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "training_logs = os.path.join('logs/train/', current_time)\n",
    "writer = tf.summary.create_file_writer(training_logs)\n",
    "\n",
    "with open(os.path.join(training_logs, 'parameters.txt'), 'w') as f:\n",
    "    f.write('learning_rate: {}\\n'.format(learning_rate))\n",
    "    f.write('encoding_layer_size: {}\\n'.format(encoding_layer_size))\n",
    "    f.write('epochs: {}\\n'.format(epochs))\n",
    "    f.write('batch_size: {}\\n'.format(batch_size))\n",
    "\n",
    "autoencoder = AutoencoderModel(encoding_layer_size=encoding_layer_size, attributes_size=len(df_numerical.columns) - 1)\n",
    "optamizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Already scaled so just setting it\n",
    "scaled_train = X_train\n",
    "scaled_train = pd.DataFrame(scaled_train).astype(np.float64)\n",
    "scaled_test = X_test\n",
    "scaled_test = pd.DataFrame(scaled_test).astype(np.float64)\n",
    "scaled_cv = X_cv\n",
    "scaled_cv = pd.DataFrame(scaled_cv).astype(np.float64)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(scaled_train.values)\n",
    "train_dataset_batches = train_dataset.shuffle(len(scaled_train)).batch(batch_size)\n",
    "\n",
    "with writer.as_default():\n",
    "    with tf.summary.record_if(True):\n",
    "        cumulative_step = 0\n",
    "        for epoch in tqdm_notebook(range(epochs)):\n",
    "            for step, cur_batch in enumerate(train_dataset_batches):\n",
    "                grad_step(loss, autoencoder, optamizer, cur_batch)\n",
    "                loss_values = loss(autoencoder, cur_batch)\n",
    "                #original = cur_batch\n",
    "                #reconstructed = autoencoder(tf.constant(cur_batch))\n",
    "                    \n",
    "                tf.summary.scalar('loss', loss_values, step=cumulative_step)\n",
    "                    \n",
    "                cumulative_step += 1\n",
    "                cv_loss_values = loss(autoencoder, scaled_cv.sample(batch_size).values)\n",
    "                tf.summary.scalar('CV-loss', cv_loss_values , step=cumulative_step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
